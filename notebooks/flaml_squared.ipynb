{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34f30c6",
   "metadata": {},
   "source": [
    "## FLAML for hp optimisation and model selection\n",
    "We use FLAML twice, first to find the best component model for each estimator, and then to optimise the estimators themselves and choose the best estimator. Here we show how it's done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7480f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # suppress sklearn deprecation warnings for now.. \n",
    "\n",
    "root_path = root_path = os.path.realpath('../..')\n",
    "data_dir = os.path.realpath(os.path.join(root_path, \"auto-causality/data\"))\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "sys.path.append(os.path.join(root_path, \"auto-causality\"))\n",
    "sys.path.append(os.path.join(root_path, \"dowhy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8de5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_causality.utils import featurize\n",
    "from auto_causality import AutoCausality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c777cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all the control parameters here\n",
    "train_size = 0.5\n",
    "test_size = None\n",
    "time_budget = 300\n",
    "num_cores = os.cpu_count() - 1\n",
    "conf_intervals = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9331f28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load raw data\n",
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/AMLab-Amsterdam/CEVAE/master/datasets/IHDP/csv/ihdp_npci_1.csv\",\n",
    "    header=None,\n",
    ")\n",
    "col = [\n",
    "    \"treatment\",\n",
    "    \"y_factual\",\n",
    "    \"y_cfactual\",\n",
    "    \"mu0\",\n",
    "    \"mu1\",\n",
    "]\n",
    "for i in range(1, 26):\n",
    "    col.append(\"x\" + str(i))\n",
    "data.columns = col\n",
    "# drop the columns we don't care about\n",
    "ignore_patterns = [\"y_cfactual\", \"mu\"]\n",
    "ignore_cols = [c for c in data.columns if any([s in c for s in ignore_patterns])]\n",
    "data = data.drop(columns=ignore_cols)\n",
    "\n",
    "\n",
    "# prepare the data\n",
    "\n",
    "treatment = \"treatment\"\n",
    "targets = [\"y_factual\"]  # it's good to allow multiple ones\n",
    "features = [c for c in data.columns if c not in [treatment] + targets]\n",
    "\n",
    "data[treatment] = data[treatment].astype(int)\n",
    "# this is a trick to bypass some DoWhy/EconML bugs\n",
    "data[\"random\"] = np.random.randint(0, 2, size=len(data))\n",
    "\n",
    "used_df = featurize(\n",
    "    data, features=features, exclude_cols=[treatment] + targets, drop_first=False,\n",
    ")\n",
    "used_features = [\n",
    "    c for c in used_df.columns if c not in ignore_cols + [treatment] + targets\n",
    "]\n",
    "\n",
    "\n",
    "# Let's treat all features as effect modifiers\n",
    "features_X = [f for f in used_features if f != \"random\"]\n",
    "features_W = [f for f in used_features if f not in features_X]\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(used_df, train_size=train_size)\n",
    "if test_size is not None:\n",
    "    test_df = test_df.sample(test_size)\n",
    "\n",
    "test_df.to_csv(os.path.join(data_dir, f\"test_{time_budget}.csv\"))\n",
    "train_df.to_csv(os.path.join(data_dir, f\"train_{time_budget}.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d234a",
   "metadata": {},
   "source": [
    "### Model fitting & scoring\n",
    "Here we fit a (selection of) model(s) to the data and score them with the ERUPT metric on held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f68c4d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 03-06 12:25:29] {326} WARNING - Using CFO for search. To use BlendSearch, run: pip install flaml[blendsearch]\n",
      "[flaml.tune.tune: 03-06 12:25:29] {447} INFO - trial 1 config: {'fit_cate_intercept': 1, 'mc_iters': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting estimators: ['backdoor.econml.dml.LinearDML', 'backdoor.econml.dml.SparseLinearDML', 'backdoor.econml.dml.CausalForestDML', 'backdoor.econml.dr.ForestDRLearner']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 03-06 12:25:31] {326} WARNING - Using CFO for search. To use BlendSearch, run: pip install flaml[blendsearch]\n",
      "[flaml.tune.tune: 03-06 12:25:31] {447} INFO - trial 1 config: {'fit_cate_intercept': 1, 'mc_iters': 0, 'n_alphas': 87, 'n_alphas_cov': 5, 'tol': 3.81e-05, 'max_iter': 18500}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Estimator: backdoor.econml.dml.LinearDML \t qini: 0.073689\n",
      " erupt: 6.522496\n",
      " qini: 0.073689\n",
      " ATE: 3.648906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 03-06 12:25:34] {326} WARNING - Using CFO for search. To use BlendSearch, run: pip install flaml[blendsearch]\n",
      "[flaml.tune.tune: 03-06 12:25:34] {447} INFO - trial 1 config: {'mc_iters': 5, 'drate': 0, 'n_estimators': 67, 'criterion': 'het', 'max_depth': 2, 'min_samples_split': 18, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.377743449018298, 'min_var_fraction_leaf': 0.26639242043080236, 'max_features': 'sqrt', 'min_impurity_decrease': 7.5621068260561595, 'max_samples': 0.013611566647889872, 'min_balancedness_tol': 0.3836583130489407, 'honest': 0, 'inference': 0, 'fit_intercept': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Estimator: backdoor.econml.dml.SparseLinearDML \t qini: 0.082383\n",
      " erupt: 6.531159\n",
      " qini: 0.082383\n",
      " ATE: 4.310090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 03-06 12:25:44] {326} WARNING - Using CFO for search. To use BlendSearch, run: pip install flaml[blendsearch]\n",
      "[flaml.tune.tune: 03-06 12:25:44] {447} INFO - trial 1 config: {'min_propensity': 3.070155442191211e-06, 'mc_iters': 1, 'n_estimators': 324, 'max_depth': 2, 'min_samples_split': 18, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.377743449018298, 'max_features': 'sqrt', 'min_impurity_decrease': 2.6639242043080236, 'max_samples': 0.378105341302808, 'min_balancedness_tol': 0.006805783323944936, 'honest': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Estimator: backdoor.econml.dml.CausalForestDML \t qini: 0.062831\n",
      " erupt: 6.571430\n",
      " qini: 0.062831\n",
      " ATE: 3.852123\n",
      "... Estimator: backdoor.econml.dr.ForestDRLearner \t qini: 0.062831\n",
      " erupt: 6.571430\n",
      " qini: 0.062831\n",
      " ATE: 3.954098\n",
      "Best estimator: backdoor.econml.dml.SparseLinearDML\n"
     ]
    }
   ],
   "source": [
    "\n",
    "estimator_list = [\"dml\",\"ForestDR\"]\n",
    "outcome = targets[0]\n",
    "auto_causality = AutoCausality(time_budget=1,components_time_budget=1,estimator_list=estimator_list, metric = 'qini')\n",
    "\n",
    "myresults = auto_causality.fit(train_df, test_df, treatment, outcome,\n",
    " features_W, features_X)\n",
    "\n",
    "print(f\"Best estimator: {auto_causality.best_estimator}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b3323bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['treatment', 'y_factual', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7',\n",
      "       'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17',\n",
      "       'x18', 'x19', 'x20', 'x21', 'x22', 'x23', 'x24', 'x25', 'random',\n",
      "       'tau'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052956f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
